import os
import json
import time # <-- Th√™m th∆∞ vi·ªán time ƒë·ªÉ t·∫°o deploy_key
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager

class DataIngestionTool:
    """
    M·ªôt c√¥ng c·ª• ƒë·ªÉ c√†o d·ªØ li·ªáu vƒÉn b·∫£n ph√°p lu·∫≠t t·ª´ trang vbpl.vn.
    """
    def __init__(self, web_link: str = 'https://vbpl.vn/pages/portal.aspx'):
        """
        Kh·ªüi t·∫°o WebDriver, t·ª± ƒë·ªông qu·∫£n l√Ω chromedriver.

        Args:
            web_link (str, optional): Link c·ªßa trang web c·∫ßn c√†o. M·∫∑c ƒë·ªãnh l√† 'https://vbpl.vn/pages/portal.aspx'.
        """
        self.webpage_link = web_link
        try:
            self.service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=self.service)
            print("üü¢ T·∫°o driver th√†nh c√¥ng.")
        except Exception as e:
            print(f"‚≠ï L·ªói khi t·∫°o driver: {e}")
            self.driver = None

    def ingest_data(self, keyword: str, output_dir: str):
        """
        T·∫£i v·ªÅ to√†n b·ªô vƒÉn b·∫£n ph√°p lu·∫≠t v√† metadata li√™n quan t·ªõi keyword.

        Args:
            keyword (str): T·ª´ kh√≥a t√¨m ki·∫øm.
            output_dir (str): Th∆∞ m·ª•c ƒë·ªÉ l∆∞u file JSON k·∫øt qu·∫£.
        """
        if not self.driver:
            print("Driver ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. D·ª´ng th·ª±c thi.")
            return

        self.driver.get(self.webpage_link)
        keyword = keyword.strip()
        all_documents = []

        print(f'\n============== B·∫Øt ƒë·∫ßu t√¨m ki·∫øm: "{keyword}" ==============')
        try:
            search_box = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.ID, "AdvanceKeyword"))
            )
            search_box.clear()
            search_box.send_keys(keyword)
            search_box.send_keys(Keys.ENTER)

            result_count_object = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.XPATH, '//span[contains(text(), "T√¨m th·∫•y")]/strong'))
            )
            number_of_results = int(result_count_object.text)
            print(f"üî¨ ƒê√£ t√¨m th·∫•y {number_of_results} vƒÉn b·∫£n ph√°p lu·∫≠t.")
            if number_of_results == 0:
                return
        except TimeoutException:
            print(f"‚≠ï Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o cho t·ª´ kh√≥a '{keyword}' ho·∫∑c trang t·∫£i qu√° l√¢u.")
            return
        except Exception as e:
            print(f"‚≠ï L·ªói khi t√¨m ki·∫øm: {e}")
            return

        page_number = 1
        
        while True:
            try:
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CLASS_NAME, "listLaw"))
                )
                ul_element = self.driver.find_element(By.CLASS_NAME, "listLaw")
                li_elements = ul_element.find_elements(By.TAG_NAME, "li")
                print(f"üîç ƒêang x·ª≠ l√Ω trang {page_number} v·ªõi {len(li_elements)} k·∫øt qu·∫£...")
                
                page_data = []
                for li in li_elements:
                    try:
                        a_tag = li.find_element(By.CSS_SELECTOR, "p.title > a")
                        detail_link = a_tag.get_attribute("href")
                        
                        doc_info = {
                            "T√™n vƒÉn b·∫£n": a_tag.text.strip(),
                            "Link chi ti·∫øt": detail_link,
                            "M√¥ t·∫£": li.find_element(By.CSS_SELECTOR, "div.des > p").text.strip(),
                            "PDF": li.find_element(By.CSS_SELECTOR, "li.source > a").get_attribute("href"),
                            "Ban h√†nh": li.find_element(By.CSS_SELECTOR, "div.right > p.green:nth-of-type(1)").text.split(":", 1)[-1].strip(),
                            "Hi·ªáu l·ª±c": li.find_element(By.CSS_SELECTOR, "div.right > p.green:nth-of-type(2)").text.split(":", 1)[-1].strip(),
                            "Tr·∫°ng th√°i": li.find_element(By.CSS_SELECTOR, "div.right > p.red").text.split(":", 1)[-1].strip(),
                            "N·ªôi dung": ""
                        }
                        page_data.append(doc_info)
                    except NoSuchElementException:
                        continue
                        
                main_window = self.driver.current_window_handle
                for doc in page_data:
                    print(f"  üîó ƒêang l·∫•y n·ªôi dung cho: {doc['T√™n vƒÉn b·∫£n'][:50]}...")
                    self.driver.switch_to.new_window('tab')
                    self.driver.get(doc["Link chi ti·∫øt"])
                    try:
                        content_div = WebDriverWait(self.driver, 10).until(
                            EC.presence_of_element_located((By.ID, "toanvancontent"))
                        )
                        doc["N·ªôi dung"] = content_div.text
                    except TimeoutException:
                        print(f"  ‚ùå Kh√¥ng t√¨m th·∫•y n·ªôi dung cho vƒÉn b·∫£n n√†y.")
                    finally:
                        self.driver.close()
                        self.driver.switch_to.window(main_window)

                all_documents.extend(page_data)

                old_ul_element = self.driver.find_element(By.CLASS_NAME, "listLaw")
                next_page_link = self.driver.find_element(By.XPATH, f"//div[@class='paging']//a[text()='{page_number + 1}']")
                next_page_link.click()
                
                WebDriverWait(self.driver, 10).until(EC.staleness_of(old_ul_element))
                page_number += 1

            except NoSuchElementException:
                print("‚úÖ ƒê√£ x·ª≠ l√Ω h·∫øt c√°c trang k·∫øt qu·∫£ cho t·ª´ kh√≥a n√†y.")
                break
            except Exception as e:
                print(f"‚≠ï L·ªói khi x·ª≠ l√Ω trang {page_number}: {e}. D·ª´ng l·∫°i.")
                break
        
        self.save_to_json(all_documents, keyword, output_dir)

    def save_to_json(self, data: list, keyword: str, output_dir: str):
        """L∆∞u d·ªØ li·ªáu v√†o file JSON."""
        if not data:
            print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u cho t·ª´ kh√≥a n√†y.")
            return

        os.makedirs(output_dir, exist_ok=True)
        
        safe_keyword = "".join(c for c in keyword if c.isalnum() or c in (' ', '_')).rstrip()
        filename = f"metadata_law_{safe_keyword.replace(' ', '_')}.json"
        json_path = os.path.join(output_dir, filename)

        df = pd.DataFrame(data)
        df.to_json(json_path, orient="records", force_ascii=False, indent=4)
        print(f"üíæ ƒê√£ l∆∞u th√†nh c√¥ng {len(data)} vƒÉn b·∫£n v√†o file: {json_path}")

    def quit(self):
        """ƒê√≥ng tr√¨nh duy·ªát."""
        if self.driver:
            self.driver.quit()
            print("\nüö™ ƒê√£ ƒë√≥ng driver.")


if __name__ == '__main__':
    KEYWORDS_TO_SEARCH = [
        "kh√°m ch·ªØa b·ªánh",
        "b·∫£o hi·ªÉm y t·∫ø",
        "thanh to√°n b·∫£o hi·ªÉm",
        "chuy·ªÉn tuy·∫øn",
        "gi·∫•y chuy·ªÉn tuy·∫øn",
        "ƒëƒÉng k√Ω kh√°m ch·ªØa b·ªánh ban ƒë·∫ßu",
        "m√£ ƒë·ªãnh danh y t·∫ø",
        "th·∫ª b·∫£o hi·ªÉm y t·∫ø",
        "h·ªì s∆° b·ªánh √°n",
        "quy tr√¨nh kh√°m b·ªánh",
        "x√©t nghi·ªám",
        "vi·ªán ph√≠",
        "thu·ªëc v√† v·∫≠t t∆∞ y t·∫ø",
        "thanh to√°n tr·ª±c tuy·∫øn",
        "ƒë·ªìng chi tr·∫£",
        "quy·ªÅn l·ª£i ng∆∞·ªùi b·ªánh",
    ]

    deploy_key = time.strftime("%Y%m%d-%H%M%S")
    print(f"üîë Deploy key cho l·∫ßn ch·∫°y n√†y: {deploy_key}")

    BASE_OUTPUT_DIRECTORY = os.path.join(r".\data\legal_documents\raw", deploy_key)
    
    # --- PH·∫¶N TH·ª∞C THI ---
    ingestion_tool = DataIngestionTool()
    
    try:
        if ingestion_tool.driver:
            for keyword in KEYWORDS_TO_SEARCH:
                ingestion_tool.ingest_data(keyword=keyword, output_dir=BASE_OUTPUT_DIRECTORY)
    finally:
        ingestion_tool.quit()
