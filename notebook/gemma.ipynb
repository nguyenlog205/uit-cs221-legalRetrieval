{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db2bb9ad",
   "metadata": {},
   "source": [
    "# Training gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dedac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig, \n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    prepare_model_for_kbit_training, \n",
    "    TaskType\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db135b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FinetuneConfig:\n",
    "    # Model g·ªëc c·ªßa Google, kh√¥ng d√πng h√†ng Unsloth n·ªØa\n",
    "    model_name: str = \"google/gemma-2-2b\" \n",
    "    max_seq_length: int = 2048\n",
    "    \n",
    "    # LoRA Params\n",
    "    lora_rank: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    # Native PEFT c·∫ßn list target modules ch√≠nh x√°c\n",
    "    target_modules: List[str] = field(default_factory=lambda: [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ])\n",
    "    \n",
    "    # Train Params\n",
    "    learning_rate: float = 2e-4\n",
    "    batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    max_steps: int = 60\n",
    "    warmup_steps: int = 5\n",
    "    logging_steps: int = 1\n",
    "    seed: int = 3407\n",
    "    \n",
    "    output_dir: str = \"models/gemma_native_finetuned\"\n",
    "    dataset_path: str = \"../data/question_answer/datasetdone.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eab08f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = FinetuneConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c515322",
   "metadata": {},
   "source": [
    "## 1. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa8f9dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, TaskType\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc008172",
   "metadata": {},
   "source": [
    "### 1.1. LoRA configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e8bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=\"all-linear\",\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b3b9b4",
   "metadata": {},
   "source": [
    "### 1.2. Quantization configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98bc9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,              \n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a37a250d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading Model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66da835251141298840a1b921e6dd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"üìÇ Loading Model: {config.model_name}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85cf378e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading Tokenizer: google/gemma-2-2b\n"
     ]
    }
   ],
   "source": [
    "print(f\"üìÇ Loading Tokenizer: {config.model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de249fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Attaching LoRA adapters...\n",
      "trainable params: 20,766,720 || all params: 2,635,108,608 || trainable%: 0.7881\n"
     ]
    }
   ],
   "source": [
    "print(\"üõ†Ô∏è Attaching LoRA adapters...\")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db3f43e",
   "metadata": {},
   "source": [
    "### 1.3. Load model and QLoRA attachment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b9077f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "def run_finetuning(\n",
    "    config: FinetuneConfig,\n",
    "    model,\n",
    "    tokenizer,\n",
    "):\n",
    "    # --- 0. CHECK CUDA & GPU INFO ---\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"üî• GPU DETECTED: {gpu_name} ({vram_gb:.2f} GB VRAM)\")\n",
    "        print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è WARNING: KH√îNG T√åM TH·∫§Y GPU! ƒêang ch·∫°y b·∫±ng CPU (s·∫Ω r·∫•t ch·∫≠m).\")\n",
    "    print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "    versioning = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_output_dir = f\"{config.output_dir}/{versioning}\"\n",
    "    print(f\"üöÄ Starting run: {versioning}\")\n",
    "\n",
    "    # --- 1. Load Data & Format ---\n",
    "    print(f\"üìö Loading Data from: {config.dataset_path}\")\n",
    "    try:\n",
    "        df = pd.read_excel(config.dataset_path)\n",
    "        if 'Column2' not in df.columns or 'Column3' not in df.columns:\n",
    "             df.columns = ['Column2', 'Column3'] + list(df.columns[2:])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói ƒë·ªçc Excel: {e}\")\n",
    "        return\n",
    "\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    def formatting_prompts_func(examples):\n",
    "        texts = [f\"<start_of_turn>user\\n{q}<end_of_turn>\\n<start_of_turn>model\\n{a}<end_of_turn>\" \n",
    "                 for q, a in zip(examples[\"Column2\"], examples[\"Column3\"])]\n",
    "        return { \"text\" : texts }\n",
    "    dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "    # --- 2. Config & Trainer Setup ---\n",
    "    print(\"‚öôÔ∏è Setting up Trainer...\")\n",
    "    \n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=run_output_dir,\n",
    "        \n",
    "        # === DATASET & LENGTH (Quan tr·ªçng) ===\n",
    "        dataset_text_field=\"text\",\n",
    "        packing=False,\n",
    "        \n",
    "        # === GPU & PERFORMANCE ===\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        warmup_steps=config.warmup_steps,\n",
    "        max_steps=config.max_steps,\n",
    "        learning_rate=config.learning_rate,\n",
    "        fp16=True,                # B·∫Øt bu·ªôc True ƒë·ªÉ ch·∫°y t·ªët tr√™n GPU T4/Consumer\n",
    "        # bf16=True,              # B·∫≠t c√°i n√†y n·ªÅu d√πng RTX 3090/4090 ho·∫∑c A100 (t·∫Øt fp16 ƒëi)\n",
    "        optim=\"paged_adamw_8bit\", # Optimizer ti·∫øt ki·ªám VRAM\n",
    "        \n",
    "        # === TI·∫æN TR√åNH & LOGGING (Hi·ªÉn th·ªã thanh ch·∫°y) ===\n",
    "        logging_steps=1,          # In log sau M·ªñI b∆∞·ªõc (ƒë·ªÉ th·∫•y n√≥ ch·∫°y ngay)\n",
    "        logging_first_step=True,  # In ngay b∆∞·ªõc ƒë·∫ßu ti√™n\n",
    "        report_to=\"none\",         # In ra m√†n h√¨nh console (kh√¥ng g·ª≠i l√™n wandb)\n",
    "        disable_tqdm=False,       # ƒê·∫£m b·∫£o thanh loading bar hi·ªán l√™n\n",
    "        save_strategy=\"no\",       # Kh√¥ng save checkpoint r√°c gi·ªØa ch·ª´ng cho nh·∫π ·ªï c·ª©ng\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        processing_class=tokenizer,\n",
    "        args=sft_config,\n",
    "    )\n",
    "\n",
    "    # --- 3. RUN ---\n",
    "    print(\"\\nüî• TRAINING STARTED... (N·∫øu th·∫•y Loss gi·∫£m l√† ngon)\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå L·ªói khi train: {e}\")\n",
    "        print(\"üí° G·ª£i √Ω: N·∫øu l·ªói CUDA OOM (Out of Memory), h√£y gi·∫£m batch_size ho·∫∑c max_seq_length.\")\n",
    "        return\n",
    "\n",
    "    # --- 4. Save ---\n",
    "    print(f\"\\nüíæ Saving adapter to {run_output_dir}...\")\n",
    "    trainer.save_model(run_output_dir)\n",
    "    tokenizer.save_pretrained(run_output_dir)\n",
    "    print(\"‚úÖ DONE! TRAINING COMPLETED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5b3da18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "üî• GPU DETECTED: NVIDIA GeForce RTX 3050 6GB Laptop GPU (6.44 GB VRAM)\n",
      "   CUDA Version: 12.1\n",
      "========================================\n",
      "\n",
      "üöÄ Starting run: 20251129-180733\n",
      "üìö Loading Data from: ../data/question_answer/datasetdone.xlsx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7ac0e930154bce9d26e3aa49ed406e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/275 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Setting up Trainer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2f3a71a0b1402aae079b20b4aba8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/275 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5531dd826dcb4ec7abf6951db77c2ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/275 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ebb402fc3e4c0ea2326dd8cd3318d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/275 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• TRAINING STARTED... (N·∫øu th·∫•y Loss gi·∫£m l√† ngon)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 1:58:21, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.768200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.354400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.328200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.311100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.192400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.706700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.991200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.148800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.993800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.652300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.007100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.670400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.613600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.631900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.775500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.618000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.775700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.930100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.806100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.697800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.644300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.689800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.548800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.569800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.752300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.716700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.770900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.420900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.513700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.721400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.493600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.461100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.514200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.356700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.229700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.227200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.151100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.185400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.197200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.316000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.261300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.570500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.309800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.206500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.357400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.482900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving adapter to models/gemma_native_finetuned/20251129-180733...\n",
      "‚úÖ DONE! TRAINING COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Kh·ªüi t·∫°o config\n",
    "    conf = FinetuneConfig()\n",
    "    # Ch·∫°y\n",
    "    run_finetuning(\n",
    "        conf,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-private-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
