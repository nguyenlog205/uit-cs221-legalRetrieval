# File: chunking.py
# Má»¥c Ä‘Ã­ch: Äá»c dá»¯ liá»‡u, sá»­ dá»¥ng module preprocessing Ä‘á»ƒ lÃ m sáº¡ch,
# sau Ä‘Ã³ thá»±c hiá»‡n chunking vÃ  lÆ°u má»—i chunk ra má»™t file riÃªng.

import json
import re
import os
from typing import List, Dict

# Import hÃ m cáº§n thiáº¿t tá»« file preprocessing.py
from _preprocessing import preprocessing_batch_news

def structural_chunking(document: Dict, chunk_level: str = 'khoan') -> List[Dict]:
    """
    Thá»±c hiá»‡n chia nhá» vÄƒn báº£n theo cáº¥u trÃºc (Äiá»u, Khoáº£n).
    """
    if chunk_level not in ['dieu', 'khoan']:
        raise ValueError("chunk_level pháº£i lÃ  'dieu' hoáº·c 'khoan'.")

    text = document.get("Ná»™i dung", "")
    if not text:
        return []

    base_metadata = {
        "source_doc_name": document.get("TÃªn vÄƒn báº£n"),
        "source_link": document.get("Link chi tiáº¿t"),
        "status": document.get("Tráº¡ng thÃ¡i"),
        "effective_date": document.get("Hiá»‡u lá»±c"),
        "issued_date": document.get("Ban hÃ nh"),
    }

    chunks = []
    articles = re.split(r'(?=^Äiá»u \w+.*)', text, flags=re.MULTILINE)

    for article_text in articles:
        article_text = article_text.strip()
        if not article_text:
            continue

        article_lines = article_text.split('\n')
        article_title = article_lines[0]
        match = re.match(r'Äiá»u (\w+)', article_title)
        article_id_str = match.group(1) if match else 'unknown'

        if chunk_level == 'dieu':
            metadata = base_metadata.copy()
            metadata.update({
                "structure": f"Äiá»u {article_id_str}",
                "chunk_id": f"{base_metadata['source_doc_name']}_dieu_{article_id_str}"
            })
            chunks.append({"content": article_text, "metadata": metadata})
            continue

        clauses = re.split(r'(?=^\d+\. )', article_text, flags=re.MULTILINE)
        article_header = clauses[0]
        
        if len(clauses) <= 1:
            metadata = base_metadata.copy()
            metadata.update({
                "structure": f"Äiá»u {article_id_str}",
                "chunk_id": f"{base_metadata['source_doc_name']}_dieu_{article_id_str}"
            })
            chunks.append({"content": article_header, "metadata": metadata})
        else:
            for i, clause_text in enumerate(clauses):
                if i == 0: continue
                clause_text = clause_text.strip()
                if not clause_text: continue
                clause_match = re.match(r'(\d+)\.', clause_text)
                clause_id_str = clause_match.group(1) if clause_match else f"p{i}"
                chunk_content = f"{article_header.strip()}\n{clause_text}"
                metadata = base_metadata.copy()
                metadata.update({
                    "structure": f"Äiá»u {article_id_str} - Khoáº£n {clause_id_str}",
                    "chunk_id": f"{base_metadata['source_doc_name']}_dieu_{article_id_str}_khoan_{clause_id_str}"
                })
                chunks.append({"content": chunk_content, "metadata": metadata})
    return chunks

def sanitize_filename(name: str) -> str:
    """LÃ m sáº¡ch chuá»—i Ä‘á»ƒ táº¡o tÃªn file/thÆ° má»¥c há»£p lá»‡."""
    # Thay tháº¿ dáº¥u / báº±ng dáº¥u -
    name = name.replace('/', '-')
    # Loáº¡i bá» cÃ¡c kÃ½ tá»± khÃ´ng há»£p lá»‡ khÃ¡c
    return re.sub(r'[\\?%*:|"<>]', '_', name)

def save_chunks_to_separate_files(all_chunks: List[Dict], base_output_dir: str):
    """
    LÆ°u má»—i chunk vÃ o má»™t file JSON riÃªng biá»‡t, Ä‘Æ°á»£c tá»• chá»©c trong cÃ¡c
    thÆ° má»¥c con theo tÃªn vÄƒn báº£n gá»‘c.
    """
    if not os.path.exists(base_output_dir):
        os.makedirs(base_output_dir)
        print(f"ğŸ“ ÄÃ£ táº¡o thÆ° má»¥c chÃ­nh: '{base_output_dir}'")

    count = 0
    for chunk in all_chunks:
        try:
            metadata = chunk.get("metadata", {})
            doc_name = metadata.get("source_doc_name", "unknown_document")
            chunk_id = metadata.get("chunk_id", f"chunk_{count+1}")

            # Táº¡o tÃªn thÆ° má»¥c vÃ  tÃªn file há»£p lá»‡
            sanitized_doc_name = sanitize_filename(doc_name)
            sanitized_chunk_id = sanitize_filename(chunk_id)

            # Táº¡o Ä‘Æ°á»ng dáº«n thÆ° má»¥c cho vÄƒn báº£n
            doc_dir = os.path.join(base_output_dir, sanitized_doc_name)
            if not os.path.exists(doc_dir):
                os.makedirs(doc_dir)

            # Táº¡o Ä‘Æ°á»ng dáº«n file cho chunk
            output_filepath = os.path.join(doc_dir, f"{sanitized_chunk_id}.json")

            # LÆ°u chunk vÃ o file
            with open(output_filepath, 'w', encoding='utf-8') as f:
                json.dump(chunk, f, ensure_ascii=False, indent=4)
            
            count += 1
        except Exception as e:
            print(f"âŒ Gáº·p lá»—i khi lÆ°u chunk: {chunk.get('metadata', {}).get('chunk_id')}. Lá»—i: {e}")
            
    print(f"âœ… ÄÃ£ lÆ°u thÃ nh cÃ´ng {count} chunk vÃ o cÃ¡c file riÃªng trong thÆ° má»¥c '{base_output_dir}'.")

# ==============================================================================
# PHáº¦N THá»°C THI CHÃNH
# ==============================================================================
if __name__ == "__main__":
    input_file = r'data\processed\preprocessed_data.json'
    chunks_output_dir = r'data\processed'
    
    try:
        # 1. Náº¡p dá»¯ liá»‡u
        with open(input_file, 'r', encoding='utf-8') as f:
            all_documents = json.load(f)
        print(f"ğŸ“‚ ÄÃ£ Ä‘á»c thÃ nh cÃ´ng {len(all_documents)} vÄƒn báº£n tá»« file '{input_file}'.")
        
        # 2. Tiá»n xá»­ lÃ½ dá»¯ liá»‡u (sá»­ dá»¥ng hÃ m tá»« file preprocessing.py)
        print("\nâ³ Báº¯t Ä‘áº§u tiá»n xá»­ lÃ½ dá»¯ liá»‡u...")
        preprocessed_documents = preprocessing_batch_news(all_documents)
        print("âœ”ï¸ Tiá»n xá»­ lÃ½ hoÃ n táº¥t.")

        # 3. Thá»±c hiá»‡n chunking cho táº¥t cáº£ vÄƒn báº£n
        print("\nğŸ”ª Báº¯t Ä‘áº§u chia nhá» (chunking) vÄƒn báº£n...")
        all_chunks = []
        for doc in preprocessed_documents:
            if doc.get("Ná»™i dung"):
                chunks = structural_chunking(doc, chunk_level='khoan')
                all_chunks.extend(chunks)
        
        print(f"âœ”ï¸ Chia nhá» hoÃ n táº¥t: Táº¡o ra Ä‘Æ°á»£c tá»•ng cá»™ng {len(all_chunks)} chunks.")

        # 4. LÆ°u káº¿t quáº£ chunking ra cÃ¡c file riÃªng
        print("\nğŸ’¾ Báº¯t Ä‘áº§u lÆ°u cÃ¡c chunks ra file...")
        save_chunks_to_separate_files(all_chunks, chunks_output_dir)

    except FileNotFoundError:
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file '{input_file}'. Vui lÃ²ng Ä‘áº£m báº£o file nÃ y náº±m cÃ¹ng thÆ° má»¥c.")
    except Exception as e:
        print(f"âŒ ÄÃ£ xáº£y ra lá»—i khÃ´ng mong muá»‘n: {e}")