import os
import uvicorn
import logging
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional, List

# --- Import c√°c Module Agents ---
from src.agents.intent_classifier import IntentClassifier 
from src.agents.database_retriever import DatabaseRetriever
from src.agents.specialized_generator import SpecificGenerator
from src.agents.general_generator import GeneralGenerator
from src.utils import load_env

# --- C·∫•u h√¨nh Logging ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Data Models ---
class ChatRequest(BaseModel):
    query: str
    session_id: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    intent: str
    source_documents: Optional[List[str]] = None

# --- Global State ---
agents = {}

# --- Lifespan Manager ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("--- KH·ªûI ƒê·ªòNG H·ªÜ TH·ªêNG RAG (CLOUD API MODE) ---")
    
    # 1. Load Config t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
    groq_api_key = load_env("GROQ_API_KEY")
    
    # Config cho Model tr·∫£ l·ªùi ch√≠nh
    llm_api_key = load_env("LLM_API_KEY")
    llm_api_url = load_env("LLM_API_URL")
    llm_model_name = load_env("LLM_MODEL_NAME")

    # --- S·ª¨A ·ªû ƒê√ÇY: B·∫Øt bu·ªôc ph·∫£i c√≥ URL API, kh√¥ng fallback v·ªÅ localhost n·ªØa ---
    if not llm_api_url:
        # N·∫øu thi·∫øu config, m·∫∑c ƒë·ªãnh d√πng Groq lu√¥n ch·ª© kh√¥ng d√πng Local
        logger.warning("‚ö†Ô∏è Ch∆∞a c·∫•u h√¨nh LLM_API_URL. T·ª± ƒë·ªông set v·ªÅ Groq API.")
        llm_api_url = "https://api.groq.com/openai/v1/chat/completions"
        if not llm_model_name:
            llm_model_name = "llama-3.1-8b-instant"

    try:
        # 2. Kh·ªüi t·∫°o Intent Classifier
        logger.info("Initializing Intent Classifier...")
        agents["classifier"] = IntentClassifier(api_key=groq_api_key)

        # 3. Kh·ªüi t·∫°o General Generator
        logger.info("Initializing General Generator...")
        agents["general_gen"] = GeneralGenerator(api_key=groq_api_key)

        # 4. Kh·ªüi t·∫°o Specialized Generator
        logger.info(f"Initializing Specialized Generator pointing to: {llm_api_url} | Model: {llm_model_name}")
        
        agents["specific_gen"] = SpecificGenerator(
            api_key=llm_api_key,       
            api_url=llm_api_url,       
            model_id=llm_model_name,      
            max_output_tokens=1024
        )

        # 5. Kh·ªüi t·∫°o Database Retriever
        config_path = "configs/indexing_pipeline.yml" 
        if os.path.exists(config_path):
            logger.info(f"Initializing Database Retriever from {config_path}...")
            agents["retriever"] = DatabaseRetriever.from_config(config_path=config_path)
        else:
            logger.warning(f"Warning: Kh√¥ng t√¨m th·∫•y {config_path}. Ch·∫ø ƒë·ªô Specific c√≥ th·ªÉ b·ªã l·ªói.")
            agents["retriever"] = None
            
        logger.info("--- H·ªÜ TH·ªêNG ƒê√É S·∫¥N S√ÄNG ---")
        
    except Exception as e:
        logger.error(f"L·ªói kh·ªüi t·∫°o h·ªá th·ªëng: {e}")
        raise e

    yield 

    logger.info("Shutting down system...")
    agents.clear()

# --- Kh·ªüi t·∫°o App FastAPI ---
app = FastAPI(title="Vietnam Public Health RAG API", lifespan=lifespan)

# --- API Endpoints ---
@app.get("/health")
async def health_check():
    return {
        "status": "ok", 
        "components": list(agents.keys()),
        # Ch·ªâ hi·ªÉn th·ªã t√™n Model ƒëang d√πng tr√™n Cloud
        "current_model": os.getenv("LLM_MODEL_NAME", "Unknown Cloud Model") 
    }

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    query = request.query.strip()
    if not query:
        raise HTTPException(status_code=400, detail="C√¢u h·ªèi kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng.")

    # 1. Ph√¢n lo·∫°i
    try:
        intent = await agents["classifier"].classify(query)
    except Exception as e:
        logger.error(f"Classifier Error: {e}")
        intent = "specific"
    
    logger.info(f"Input: '{query}' | Intent: {intent}")

    # 2. General Chat
    if intent == "general":
        try:
            response_text = agents["general_gen"].generate_general(query)
            return ChatResponse(response=response_text, intent="general", source_documents=[])
        except Exception:
            intent = "specific"

    # 3. RAG Chat
    if intent == "specific":
        retriever = agents.get("retriever")
        
        if not retriever:
            return ChatResponse(response="DB ch∆∞a s·∫µn s√†ng.", intent="error")

        # 3a. Retrieve
        retrieved_docs = await retriever.retrieve(query, k=5)
        
        # 3b. Fallback
        if not retrieved_docs:
            fallback_text = agents["general_gen"].generate_fallback(query)
            return ChatResponse(response=fallback_text, intent="specific_fallback", source_documents=[])

        # 3c. Generate (API Call)
        answer = await agents["specific_gen"].generate_response(query, retrieved_docs)
        
        sources = [doc.metadata.get("source", "Unknown") for doc in retrieved_docs]
        
        return ChatResponse(
            response=answer,
            intent="specific",
            source_documents=list(set(sources))
        )

if __name__ == "__main__":
    print("üöÄ Starting RAG API Server...")
    # Port n√†y l√† port c·ªßa c√°i code Python n√†y (n√≥ ph·∫£i ch·∫°y th√¨ m·ªõi c√≥ API m√† g·ªçi)
    # Ch·ª© kh√¥ng ph·∫£i port c·ªßa Model LLM (Model LLM n·∫±m tr√™n server Groq r·ªìi)
    uvicorn.run("main_api:app", host="0.0.0.0", port=8000, reload=True)